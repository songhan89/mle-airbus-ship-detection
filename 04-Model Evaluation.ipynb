{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import kfp\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, Metrics,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-anderson",
   "metadata": {},
   "source": [
    "## Load Params and Resource Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.gcp_resource import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if GCS_BUCKET == \"\" or GCS_BUCKET is None or GCS_BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    GCS_BUCKET = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
      "PIPELINE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_root\n",
      "MODULE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_module\n",
      "DATA_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/data\n",
      "SERVING_MODEL_DIR: gs://mle_airbus_dataset/airbusmlepipeline/serving_model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('MODULE_ROOT: {}'.format(MODULE_ROOT))\n",
    "print('DATA_ROOT: {}'.format(DATA_ROOT))\n",
    "print('SERVING_MODEL_DIR: {}'.format(SERVING_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb205d-e113-4b6d-8442-af7c6d648344",
   "metadata": {},
   "source": [
    "## Model Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30cfb5ed-9684-4fd8-81be-a4bbe0259001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/evaluation/eval_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/evaluation/eval_component.py\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy,BinaryCrossentropy \n",
    "from google.cloud import storage\n",
    "from src.models.preprocessing import Augment\n",
    "from src.utils.common import *\n",
    "from src.utils.dataset import *\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--test_filepath', dest='test_filepath',\n",
    "                    default='', type=str,\n",
    "                    help='Validation data file path')\n",
    "parser.add_argument('--model_filepath', dest='model_filepath',\n",
    "                    default='', type=str,\n",
    "                    help='Model file path')\n",
    "parser.add_argument('--output', dest='metrics',\n",
    "                    default='', type=str,\n",
    "                    help='Metrics output')\n",
    "args = parser.parse_args()\n",
    "\n",
    "test_filepath: str,\n",
    "model_filepath: str,\n",
    "metrics: Output[Metrics]\n",
    "\n",
    "IMG_SHAPE=(128,128)\n",
    "GCS_BUCKET=\"mle_airbus_dataset\"\n",
    "BATCH_SIZE = 16\n",
    "EDGE_CROP = 16\n",
    "NB_EPOCHS = 10\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# downsampling inside the network\n",
    "NET_SCALING = None\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (1, 1)\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 10\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 200\n",
    "AUGMENT_BRIGHTNESS = False\n",
    "N_SAMPLE = 100\n",
    "bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "\n",
    "blob = bucket.blob(\"test.parquet\")\n",
    "blob.download_to_filename(\"test.parquet\")\n",
    "\n",
    "valid_df = pd.read_parquet(f\"test.parquet\")\n",
    "validation = tf.data.Dataset.from_tensor_slices((valid_df['ImageId'].values, valid_df['EncodedPixels'].values))\n",
    "validation = validation.shuffle(buffer_size=10)\n",
    "validation = validation.map(lambda x, y: parse_db_to_img(\"gs://mle_airbus_dataset/train_v2/\" + x, y))\n",
    "validation = validation.batch(BATCH_SIZE)\n",
    "validation = validation.map(Augment(resize_shape=IMG_SHAPE, train=False))\n",
    "validation = validation.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "model_eval = tf.keras.models.load_model(args.model_filepath, compile=False)\n",
    "model_eval.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])\n",
    "result = model_eval.evaluate(validation)\n",
    "metrics.log_metric(\"dice_coef\", (result[1]))\n",
    "metrics.log_metric(\"binary_accuracy\", (result[2]))\n",
    "metrics.log_metric(\"true_positive_rate\", (result[3]))\n",
    "    import numpy as np\n",
    "    import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-momentum",
   "metadata": {},
   "source": [
    "## Model Evaluation Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.7\",\n",
    "    output_component_file=\"./build/model_eval_test_component.yaml\")\n",
    "def model_eval_test_component(\n",
    "    metrics: Input[Metrics]\n",
    "):\n",
    "    \"\"\"\n",
    "    Unit test component that checks if the output metrics passed\n",
    "    thresholds\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    metrics_thresholds = {\n",
    "        'dice_coef': 0.1,\n",
    "        'binary_accuracy': 0.8,\n",
    "        'true_positive_rate': 0.3\n",
    "    }\n",
    "    \n",
    "    for k, v in metrics.metadata.items():\n",
    "        assert v >= metrics_thresholds[k]\n",
    "        logging.info(f\"{k}:{v}, threshold: {metrics_thresholds[k]}. Passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-catch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
