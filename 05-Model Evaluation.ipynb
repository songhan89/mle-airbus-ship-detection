{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2315087f-0db1-493d-8039-ba26f8f1b29b",
   "metadata": {},
   "source": [
    "# 05 - Model Evaluation\n",
    "\n",
    "This notebook creates two components that runs evaluation result on test data, and check if the metrics passed preset threshold before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d395c-8ba6-4648-a0e3-13d4d6db8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import kfp\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, Metrics,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-anderson",
   "metadata": {},
   "source": [
    "## Load Params and Resource Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.gcp_resource import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if GCS_BUCKET == \"\" or GCS_BUCKET is None or GCS_BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    GCS_BUCKET = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
      "PIPELINE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_root\n",
      "MODULE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_module\n",
      "DATA_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/data\n",
      "SERVING_MODEL_DIR: gs://mle_airbus_dataset/airbusmlepipeline/serving_model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('MODULE_ROOT: {}'.format(MODULE_ROOT))\n",
    "print('DATA_ROOT: {}'.format(DATA_ROOT))\n",
    "print('SERVING_MODEL_DIR: {}'.format(SERVING_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb205d-e113-4b6d-8442-af7c6d648344",
   "metadata": {},
   "source": [
    "## Model Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30cfb5ed-9684-4fd8-81be-a4bbe0259001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import ClassificationMetrics, Metrics, Output, component\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"numpy\", \"pandas\", \"google-cloud-storage\", \"fsspec\", \"pyarrow\"],\n",
    "    base_image=\"python:3.7\",\n",
    "    output_component_file=\"./build/model_eval_component.yaml\")\n",
    "def model_eval_component(\n",
    "    test_filepath: str,\n",
    "    model_filepath: str,\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import tensorflow.keras.backend as K\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.losses import binary_crossentropy,BinaryCrossentropy \n",
    "    from google.cloud import storage\n",
    "\n",
    "\n",
    "    class Augment(tf.keras.layers.Layer):\n",
    "        def __init__(self,  resize_shape=(768, 768), train=True, seed=42):\n",
    "            super().__init__()\n",
    "        # both use the same seed, so they'll make the same random changes.\n",
    "            seed = np.random.randint(1000)\n",
    "            if train:\n",
    "                self.augment_inputs = tf.keras.Sequential(\n",
    "                                        [\n",
    "                                            layers.experimental.preprocessing.RandomFlip(seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomRotation(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomHeight(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomWidth(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomZoom(0.9, seed=seed),\n",
    "                                            layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "                                            layers.experimental.preprocessing.Resizing(resize_shape[0], resize_shape[0])\n",
    "                                        ]\n",
    "                                    )\n",
    "\n",
    "                self.augment_labels = tf.keras.Sequential(\n",
    "                                        [\n",
    "                                            layers.experimental.preprocessing.RandomFlip(seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomRotation(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomHeight(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomWidth(0.1, seed=seed),\n",
    "                                            layers.experimental.preprocessing.RandomZoom(0.9, seed=seed),\n",
    "                                            layers.experimental.preprocessing.Resizing(resize_shape[0], resize_shape[0])\n",
    "                                        ]\n",
    "                                    )\n",
    "            else:\n",
    "                self.augment_inputs = tf.keras.Sequential(\n",
    "                                        [\n",
    "                                            layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "                                            layers.experimental.preprocessing.Resizing(resize_shape[0], resize_shape[0])\n",
    "                                        ]\n",
    "                                    )\n",
    "\n",
    "                self.augment_labels = tf.keras.Sequential(\n",
    "                                        [\n",
    "                                            layers.experimental.preprocessing.Resizing(resize_shape[0], resize_shape[0])\n",
    "                                        ]\n",
    "                                    )       \n",
    "\n",
    "        def call(self, inputs, labels):\n",
    "            inputs = self.augment_inputs(inputs)\n",
    "            labels = self.augment_labels(labels)\n",
    "            return inputs, labels\n",
    "    \n",
    "    def dice_coef(y_true, y_pred, smooth=1):\n",
    "        intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "        union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "        return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "    def dice_p_bce(in_gt, in_pred):\n",
    "        return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "\n",
    "    def true_positive_rate(y_true, y_pred):\n",
    "        return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "    \n",
    "    #TODO: How to improve these functions ?\n",
    "    def rle_decode_tf(mask_rle, shape=(768, 768)):\n",
    "\n",
    "        shape = tf.convert_to_tensor(shape, tf.int64)\n",
    "        size = tf.math.reduce_prod(shape)\n",
    "        # Split string\n",
    "        s = tf.strings.split(mask_rle)\n",
    "        s = tf.strings.to_number(s, tf.int64)\n",
    "        # Get starts and lengths\n",
    "        starts = s[::2] - 1\n",
    "        lens = s[1::2]\n",
    "        # Make ones to be scattered\n",
    "        total_ones = tf.reduce_sum(lens)\n",
    "        ones = tf.ones([total_ones], tf.uint8)\n",
    "        # Make scattering indices\n",
    "        r = tf.range(total_ones)\n",
    "        lens_cum = tf.math.cumsum(lens)\n",
    "        s = tf.searchsorted(lens_cum, r, 'right')\n",
    "        idx = r + tf.gather(starts - tf.pad(lens_cum[:-1], [(1, 0)]), s)\n",
    "        # Scatter ones into flattened mask\n",
    "        mask_flat = tf.scatter_nd(tf.expand_dims(idx, 1), ones, [size])\n",
    "        return tf.expand_dims(tf.transpose(tf.reshape(mask_flat, shape)), axis=2)\n",
    "\n",
    "    def parse_db_to_img(filename, label):\n",
    "        file_path = filename\n",
    "        img = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(img, channels=3)\n",
    "        label_img = rle_decode_tf(label)\n",
    "\n",
    "        return image, label_img\n",
    "\n",
    "    IMG_SHAPE=(128,128)\n",
    "    GCS_BUCKET=\"mle_airbus_dataset\"\n",
    "    BATCH_SIZE = 16\n",
    "    EDGE_CROP = 16\n",
    "    NB_EPOCHS = 10\n",
    "    GAUSSIAN_NOISE = 0.1\n",
    "    UPSAMPLE_MODE = 'SIMPLE'\n",
    "    # downsampling inside the network\n",
    "    NET_SCALING = None\n",
    "    # downsampling in preprocessing\n",
    "    IMG_SCALING = (1, 1)\n",
    "    # number of validation images to use\n",
    "    VALID_IMG_COUNT = 10\n",
    "    # maximum number of steps_per_epoch in training\n",
    "    MAX_TRAIN_STEPS = 200\n",
    "    AUGMENT_BRIGHTNESS = False\n",
    "    N_SAMPLE = 100\n",
    "    bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "\n",
    "    blob = bucket.blob(\"test.parquet\")\n",
    "    blob.download_to_filename(\"test.parquet\")\n",
    "\n",
    "    valid_df = pd.read_parquet(f\"test.parquet\")\n",
    "    validation = tf.data.Dataset.from_tensor_slices((valid_df['ImageId'].values, valid_df['EncodedPixels'].values))\n",
    "    validation = validation.shuffle(buffer_size=10)\n",
    "    validation = validation.map(lambda x, y: parse_db_to_img(\"gs://mle_airbus_dataset/train_v2/\" + x, y))\n",
    "    validation = validation.batch(BATCH_SIZE)\n",
    "    validation = validation.map(Augment(resize_shape=IMG_SHAPE, train=False))\n",
    "    validation = validation.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    model_eval = tf.keras.models.load_model(f'gs://{GCS_BUCKET}/trained_model/segm_full_200_20220626-143859/', compile=False)\n",
    "    model_eval.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])\n",
    "    result = model_eval.evaluate(validation)\n",
    "    metrics.log_metric(\"dice_coef\", (result[1]))\n",
    "    metrics.log_metric(\"binary_accuracy\", (result[2]))\n",
    "    metrics.log_metric(\"true_positive_rate\", (result[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-momentum",
   "metadata": {},
   "source": [
    "## Model Evaluation Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.7\",\n",
    "    output_component_file=\"./build/model_eval_test_component.yaml\")\n",
    "def model_eval_test_component(\n",
    "    metrics: Input[Metrics]\n",
    "):\n",
    "    \"\"\"\n",
    "    Unit test component that checks if the output metrics passed\n",
    "    thresholds\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    metrics_thresholds = {\n",
    "        'dice_coef': 0.1,\n",
    "        'binary_accuracy': 0.8,\n",
    "        'true_positive_rate': 0.3\n",
    "    }\n",
    "    \n",
    "    for k, v in metrics.metadata.items():\n",
    "        assert v >= metrics_thresholds[k]\n",
    "        logging.info(f\"{k}:{v}, threshold: {metrics_thresholds[k]}. Passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
