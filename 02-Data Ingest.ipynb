{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import kfp\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, Metrics,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-anderson",
   "metadata": {},
   "source": [
    "## Load Params and Resource Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.gcp_resource import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if GCS_BUCKET == \"\" or GCS_BUCKET is None or GCS_BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    GCS_BUCKET = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
      "PIPELINE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_root\n",
      "MODULE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_module\n",
      "DATA_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/data\n",
      "SERVING_MODEL_DIR: gs://mle_airbus_dataset/airbusmlepipeline/serving_model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('MODULE_ROOT: {}'.format(MODULE_ROOT))\n",
    "print('DATA_ROOT: {}'.format(DATA_ROOT))\n",
    "print('SERVING_MODEL_DIR: {}'.format(SERVING_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb205d-e113-4b6d-8442-af7c6d648344",
   "metadata": {},
   "source": [
    "## Data Ingest Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cfb5ed-9684-4fd8-81be-a4bbe0259001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/dataset/ingest_component.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './src/dataset/ingest_component.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8232/494616801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./src/dataset/ingest_component.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nimport requests\\nimport os\\nimport logging\\nimport pandas as pd\\nimport numpy as np\\nimport tensorflow as tf\\nfrom datetime import datetime\\nfrom sklearn.utils import resample\\nfrom google.cloud import bigquery, storage\\nfrom sklearn.model_selection import train_test_split\\nfrom google.oauth2 import service_account\\nfrom skimage.segmentation import mark_boundaries\\nfrom skimage.util import montage as montage2d\\nfrom skimage.io import imread\\nfrom skimage.segmentation import mark_boundaries\\nfrom skimage.util import montage\\nfrom skimage.morphology import label\\nfrom src.utils.dataset import *\\nfrom src.utils.common import *\\nfrom pathlib import Path\\n    \\n\\nlogging.basicConfig(level=logging.INFO)\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\'--project_id\\', dest=\\'project_id\\',\\n                    default=\\'mle-airbus-detection-smu\\', type=str,\\n                    help=\\'Project id.\\')\\nparser.add_argument(\\'--gcs_bucket\\', dest=\\'gcs_bucket\\',\\n                    default=\\'mle_airbus_dataset\\', type=str,\\n                    help=\\'GCS bucket url.\\')\\nparser.add_argument(\\'--region\\', dest=\\'region\\',\\n                    default=\\'asia-east1\\', type=str,\\n                    help=\\'Project location/region.\\')\\nparser.add_argument(\\'--table-bq\\', dest=\\'table_bq\\',\\n                    default=\\'mle-airbus-detection-smu.airbus_data.label_data\\', type=str,\\n                    help=\\'BigQuery big table for source data.\\')\\nparser.add_argument(\\'--train-output\\', dest=\\'train_output\\',\\n                    default=\\'train.txt\\', type=str,\\n                    help=\\'Filename of training parquet file\\')\\nparser.add_argument(\\'--test-output\\', dest=\\'test_output\\',\\n                    default=\\'test.txt\\', type=str,\\n                    help=\\'Filename of test parquet file\\')\\nparser.add_argument(\\'--n-truncate\\', dest=\\'n_truncate\\',\\n                    default=20000, type=int,\\n                    help=\\'Filename of test parquet file\\')\\nargs = parser.parse_args()\\n\\nPROJECT_ID = args.project_id\\nGCS_BUCKET = args.gcs_bucket\\nREGION = args.region\\nTABLE_BQ = args.table_bq\\n\\nbucket = storage.Client().bucket(GCS_BUCKET)\\nbqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\\n\\ntry: \\n    bucket = storage.Client().bucket(GCS_BUCKET)\\n    bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\\n    logging.info(f\"Connection to BigQuery table {TABLE_BQ} and GCS Bucket {GCS_BUCKET} successfully.\")\\nexcept:\\n    logging.error(\"\")\\n\\n# Download a table.\\ntable = bigquery.TableReference.from_string(\\n    TABLE_BQ\\n)\\nrows = bqclient.list_rows(\\n    table\\n)\\nmasks = rows.to_dataframe(\\n    create_bqstorage_client=True,\\n)\\n\\n#reprocessing RLE data\\nmasks = masks[:args.n_truncate]\\nmasks.replace(to_replace=[None], value=\\'\\', inplace=True)\\nmasks = masks.groupby([\\'ImageId\\'])[\\'EncodedPixels\\'].apply(lambda x: \\';\\'.join(x) if x is not None else \\';\\'.join(\\'\\')).reset_index()\\n\\nmasks[\\'ships\\'] = masks[\\'EncodedPixels\\'].map(lambda c_row: c_row.count(\";\"))\\nunique_img_ids = masks.groupby(\\'ImageId\\').agg({\\'ships\\': \\'sum\\'}).reset_index()\\nunique_img_ids[\\'has_ship\\'] = unique_img_ids[\\'ships\\'].map(lambda x: 1.0 if x>0 else 0.0)\\nunique_img_ids[\\'has_ship_vec\\'] = unique_img_ids[\\'has_ship\\'].map(lambda x: [x])\\nmasks.drop([\\'ships\\'], axis=1, inplace=True)\\nmasks.EncodedPixels = masks.EncodedPixels.apply(lambda x: merge_rle_encode(x))\\n\\ntrain_ids, valid_ids = train_test_split(unique_img_ids, \\n                 test_size = 0.3, \\n                 stratify = unique_img_ids[\\'ships\\'])\\ntrain_df = pd.merge(masks, train_ids)\\nvalid_df = pd.merge(masks, valid_ids)\\nlogging.info(train_df.shape[0], \\'training masks\\')\\nlogging.info(valid_df.shape[0], \\'validation masks\\')\\n\\ntrain_df_balanced = pd.DataFrame()\\nfor ship_num in train_df[\\'ships\\'].unique():\\n    train_df_balanced = train_df_balanced.append(resample(train_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE))\\ntrain_df_balanced.reset_index(drop=True, inplace=True)\\n\\nvalid_df_balanced = pd.DataFrame()\\nfor ship_num in valid_df[\\'ships\\'].unique():\\n    valid_df_balanced = valid_df_balanced.append(resample(valid_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE//10))\\n    \\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\\n\\ntrain_df_balanced.to_parquet(f\"train.parquet\")\\nvalid_df_balanced.to_parquet(f\"test.parquet\")\\n\\nblob = bucket.blob(\\'train.parquet\\')\\nblob.upload_from_filename(\\'train.parquet\\')\\nblob = bucket.blob(\\'test.parquet\\')\\nblob.upload_from_filename(\\'test.parquet\\')\\n\\n#return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\"\\nreturn f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2470\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './src/dataset/ingest_component.py'"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/dataset/ingest_component.py\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "from google.cloud import bigquery, storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.oauth2 import service_account\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage as montage2d\n",
    "from skimage.io import imread\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage\n",
    "from skimage.morphology import label\n",
    "from src.utils.dataset import *\n",
    "from src.utils.common import *\n",
    "from pathlib import Path\n",
    "    \n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project_id', dest='project_id',\n",
    "                    default='mle-airbus-detection-smu', type=str,\n",
    "                    help='Project id.')\n",
    "parser.add_argument('--gcs_bucket', dest='gcs_bucket',\n",
    "                    default='mle_airbus_dataset', type=str,\n",
    "                    help='GCS bucket url.')\n",
    "parser.add_argument('--region', dest='region',\n",
    "                    default='asia-east1', type=str,\n",
    "                    help='Project location/region.')\n",
    "parser.add_argument('--table-bq', dest='table_bq',\n",
    "                    default='mle-airbus-detection-smu.airbus_data.label_data', type=str,\n",
    "                    help='BigQuery big table for source data.')\n",
    "parser.add_argument('--train-output', dest='train_output',\n",
    "                    default='train.txt', type=str,\n",
    "                    help='Filename of training parquet file')\n",
    "parser.add_argument('--test-output', dest='test_output',\n",
    "                    default='test.txt', type=str,\n",
    "                    help='Filename of test parquet file')\n",
    "parser.add_argument('--n-truncate', dest='n_truncate',\n",
    "                    default=20000, type=int,\n",
    "                    help='Filename of test parquet file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "PROJECT_ID = args.project_id\n",
    "GCS_BUCKET = args.gcs_bucket\n",
    "REGION = args.region\n",
    "TABLE_BQ = args.table_bq\n",
    "\n",
    "bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "try: \n",
    "    bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "    bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "    logging.info(f\"Connection to BigQuery table {TABLE_BQ} and GCS Bucket {GCS_BUCKET} successfully.\")\n",
    "except:\n",
    "    logging.error(\"\")\n",
    "\n",
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    TABLE_BQ\n",
    ")\n",
    "rows = bqclient.list_rows(\n",
    "    table\n",
    ")\n",
    "masks = rows.to_dataframe(\n",
    "    create_bqstorage_client=True,\n",
    ")\n",
    "\n",
    "#reprocessing RLE data\n",
    "masks = masks[:args.n_truncate]\n",
    "masks.replace(to_replace=[None], value='', inplace=True)\n",
    "masks = masks.groupby(['ImageId'])['EncodedPixels'].apply(lambda x: ';'.join(x) if x is not None else ';'.join('')).reset_index()\n",
    "\n",
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: c_row.count(\";\"))\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "masks.drop(['ships'], axis=1, inplace=True)\n",
    "masks.EncodedPixels = masks.EncodedPixels.apply(lambda x: merge_rle_encode(x))\n",
    "\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['ships'])\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "logging.info(train_df.shape[0], 'training masks')\n",
    "logging.info(valid_df.shape[0], 'validation masks')\n",
    "\n",
    "train_df_balanced = pd.DataFrame()\n",
    "for ship_num in train_df['ships'].unique():\n",
    "    train_df_balanced = train_df_balanced.append(resample(train_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE))\n",
    "train_df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "valid_df_balanced = pd.DataFrame()\n",
    "for ship_num in valid_df['ships'].unique():\n",
    "    valid_df_balanced = valid_df_balanced.append(resample(valid_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE//10))\n",
    "    \n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = Path(f\"{timestamp}\")\n",
    "train_df_balanced.to_parquet(f\"train.parquet\")\n",
    "valid_df_balanced.to_parquet(f\"test.parquet\")\n",
    "\n",
    "try:\n",
    "    blob = bucket.blob(f\"output_path.join('train.parquet')\")\n",
    "    blob.upload_from_filename('train.parquet')\n",
    "    blob = bucket.blob(f\"output_path.join('test.parquet')\")\n",
    "    blob.upload_from_filename('test.parquet')\n",
    "    logging.info(\"File uploaded to GCS bucket successfully.\")\n",
    "except:\n",
    "    logging.error(\"File upload to GCS Bucket failed!\")\n",
    "\n",
    "#return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\"\n",
    "return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe820780-476c-48a5-b1a8-c38f315df370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build/test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "str(Path(\"./build/test\").relative_to(\"./\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6263-da02-4695-8fee-0c40e7659a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
