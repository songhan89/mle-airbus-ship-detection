{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98375584-e2f1-4907-9cea-ebfae046dd70",
   "metadata": {},
   "source": [
    "# 02 - Data Ingest \n",
    "\n",
    "This notebook creates a data ingestion component that loads data from BigQuery, preprocess and save the result as a parquet temp file on GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3a051-5c36-4a5a-a9dd-ce88747c7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import kfp\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, Metrics,\n",
    "                        OutputPath, component)\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-anderson",
   "metadata": {},
   "source": [
    "## Load Params and Resource Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.gcp_resource import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if GCS_BUCKET == \"\" or GCS_BUCKET is None or GCS_BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    GCS_BUCKET = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
      "PIPELINE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_root\n",
      "MODULE_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/pipeline_module\n",
      "DATA_ROOT: gs://mle_airbus_dataset/airbusmlepipeline/data\n",
      "SERVING_MODEL_DIR: gs://mle_airbus_dataset/airbusmlepipeline/serving_model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n",
    "print('MODULE_ROOT: {}'.format(MODULE_ROOT))\n",
    "print('DATA_ROOT: {}'.format(DATA_ROOT))\n",
    "print('SERVING_MODEL_DIR: {}'.format(SERVING_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb205d-e113-4b6d-8442-af7c6d648344",
   "metadata": {},
   "source": [
    "## Data Ingest Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f70fb1-02d4-461f-a1ac-c98e42a7b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-storage\", \"google-cloud-bigquery\", \"tensorflow\", \n",
    "                                \"sklearn\", \"pandas\", \"scikit-image\", \"db-dtypes\", \"google-auth\",\n",
    "                               \"fsspec\", \"pyarrow\"],\n",
    "           output_component_file=\"./build/import_file_component.yaml\",)\n",
    "def import_file_component(\n",
    "    project_dict: dict\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"train_data_fpath\", str),  # Return parameter.\n",
    "        (\"test_data_fpath\", str),  # Return generic Artifact.\n",
    "    ],\n",
    "    ):\n",
    "\n",
    "    import requests\n",
    "    import os\n",
    "    import logging\n",
    "    from sklearn.utils import resample\n",
    "    from google.cloud import bigquery, storage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from google.oauth2 import service_account\n",
    "    from skimage.segmentation import mark_boundaries\n",
    "    from skimage.util import montage as montage2d\n",
    "    from skimage.io import imread\n",
    "    from skimage.segmentation import mark_boundaries\n",
    "    from skimage.util import montage\n",
    "    from skimage.morphology import label\n",
    "\n",
    "    #TODO: How to improve these functions ?\n",
    "    def rle_decode_tf(mask_rle, shape=(768, 768)):\n",
    "\n",
    "        shape = tf.convert_to_tensor(shape, tf.int64)\n",
    "        size = tf.math.reduce_prod(shape)\n",
    "        # Split string\n",
    "        s = tf.strings.split(mask_rle)\n",
    "        s = tf.strings.to_number(s, tf.int64)\n",
    "        # Get starts and lengths\n",
    "        starts = s[::2] - 1\n",
    "        lens = s[1::2]\n",
    "        # Make ones to be scattered\n",
    "        total_ones = tf.reduce_sum(lens)\n",
    "        ones = tf.ones([total_ones], tf.uint8)\n",
    "        # Make scattering indices\n",
    "        r = tf.range(total_ones)\n",
    "        lens_cum = tf.math.cumsum(lens)\n",
    "        s = tf.searchsorted(lens_cum, r, 'right')\n",
    "        idx = r + tf.gather(starts - tf.pad(lens_cum[:-1], [(1, 0)]), s)\n",
    "        # Scatter ones into flattened mask\n",
    "        mask_flat = tf.scatter_nd(tf.expand_dims(idx, 1), ones, [size])\n",
    "        return tf.expand_dims(tf.transpose(tf.reshape(mask_flat, shape)), axis=2)\n",
    "\n",
    "    def multi_rle_encode(img):\n",
    "        labels = label(img[:, :, 0])\n",
    "        return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "    # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "    def rle_encode(img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "\n",
    "    def rle_decode(mask_rle, shape=(768, 768)):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T   # Needed to align to RLE direction\n",
    "\n",
    "    def masks_as_image(in_mask_list):\n",
    "        #in_mask_list = tf.compat.as_str_any(in_mask_list)\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def merge_rle_encode(mask_rle, shape=(768, 768)):\n",
    "        img = np.zeros(shape=shape, dtype=np.uint8)\n",
    "\n",
    "        for rle in mask_rle.split(\";\"):\n",
    "            img += rle_decode(rle)\n",
    "\n",
    "        return rle_encode(img)\n",
    "\n",
    "    def parse_db_to_img(filename, label):\n",
    "        file_path = filename\n",
    "        img = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(img, channels=3)\n",
    "        label_img = rle_decode_tf(label)\n",
    "\n",
    "        return image, label_img\n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    EDGE_CROP = 16\n",
    "    NB_EPOCHS = 10\n",
    "    GAUSSIAN_NOISE = 0.1\n",
    "    UPSAMPLE_MODE = 'SIMPLE'\n",
    "    # downsampling inside the network\n",
    "    NET_SCALING = None\n",
    "    # downsampling in preprocessing\n",
    "    IMG_SCALING = (1, 1)\n",
    "    # number of validation images to use\n",
    "    VALID_IMG_COUNT = 10\n",
    "    # maximum number of steps_per_epoch in training\n",
    "    MAX_TRAIN_STEPS = 200\n",
    "    AUGMENT_BRIGHTNESS = False\n",
    "    N_SAMPLE = 100\n",
    "    IMG_SHAPE = (128, 128)\n",
    "\n",
    "    PROJECT_ID = project_dict['PROJECT_ID']\n",
    "    GCS_BUCKET = project_dict['GCS_BUCKET']\n",
    "    REGION = project_dict['REGION']\n",
    "    TABLE_BQ = project_dict['TABLE_BQ']\n",
    "    \n",
    "    try: \n",
    "        bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "        bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "        logging.info(f\"Connection to BigQuery table {TABLE_BQ} and GCS Bucket {GCS_BUCKET} successfully.\")\n",
    "    except:\n",
    "        logging.info(f\"Connection to BigQuery table {TABLE_BQ} and GCS Bucket {GCS_BUCKET} failed.\")\n",
    "\n",
    "    # Download a table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        TABLE_BQ\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    masks = rows.to_dataframe(\n",
    "        # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "        # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "        # API is used by default.\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    "    \n",
    "    masks = masks[:20000]\n",
    "    masks.replace(to_replace=[None], value='', inplace=True)\n",
    "    masks = masks.groupby(['ImageId'])['EncodedPixels'].apply(lambda x: ';'.join(x) if x is not None else ';'.join('')).reset_index()\n",
    "    \n",
    "    masks['ships'] = masks['EncodedPixels'].map(lambda c_row: c_row.count(\";\"))\n",
    "    unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "    unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "    unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "    masks.drop(['ships'], axis=1, inplace=True)\n",
    "    unique_img_ids.sample(5)\n",
    "    masks.EncodedPixels = masks.EncodedPixels.apply(lambda x: merge_rle_encode(x))\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                     test_size = 0.3, \n",
    "                     stratify = unique_img_ids['ships'])\n",
    "    train_df = pd.merge(masks, train_ids)\n",
    "    valid_df = pd.merge(masks, valid_ids)\n",
    "    print(train_df.shape[0], 'training masks')\n",
    "    print(valid_df.shape[0], 'validation masks')\n",
    "    \n",
    "    train_df_balanced = pd.DataFrame()\n",
    "    for ship_num in train_df['ships'].unique():\n",
    "        train_df_balanced = train_df_balanced.append(resample(train_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE))\n",
    "    train_df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    valid_df_balanced = pd.DataFrame()\n",
    "    for ship_num in valid_df['ships'].unique():\n",
    "        valid_df_balanced = valid_df_balanced.append(resample(valid_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE//10))\n",
    "\n",
    "    train_df_balanced.to_parquet(f\"train.parquet\")\n",
    "    valid_df_balanced.to_parquet(f\"test.parquet\")\n",
    "    \n",
    "    blob = bucket.blob('train.parquet')\n",
    "    blob.upload_from_filename('train.parquet')\n",
    "    blob = bucket.blob('test.parquet')\n",
    "    blob.upload_from_filename('test.parquet')\n",
    "    \n",
    "    return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfb5ed-9684-4fd8-81be-a4bbe0259001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/dataset/ingest_component.py\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "from google.cloud import bigquery, storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.oauth2 import service_account\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage as montage2d\n",
    "from skimage.io import imread\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage\n",
    "from skimage.morphology import label\n",
    "from src.utils.dataset import *\n",
    "from src.utils.common import *\n",
    "from pathlib import Path\n",
    "    \n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project_id', dest='project_id',\n",
    "                    default='mle-airbus-detection-smu', type=str,\n",
    "                    help='Project id.')\n",
    "parser.add_argument('--gcs_bucket', dest='gcs_bucket',\n",
    "                    default='mle_airbus_dataset', type=str,\n",
    "                    help='GCS bucket url.')\n",
    "parser.add_argument('--region', dest='region',\n",
    "                    default='asia-east1', type=str,\n",
    "                    help='Project location/region.')\n",
    "parser.add_argument('--table-bq', dest='table_bq',\n",
    "                    default='mle-airbus-detection-smu.airbus_data.label_data', type=str,\n",
    "                    help='BigQuery big table for source data.')\n",
    "parser.add_argument('--train-output', dest='train_output',\n",
    "                    default='train.txt', type=str,\n",
    "                    help='Filename of training parquet file')\n",
    "parser.add_argument('--test-output', dest='test_output',\n",
    "                    default='test.txt', type=str,\n",
    "                    help='Filename of test parquet file')\n",
    "parser.add_argument('--n-truncate', dest='n_truncate',\n",
    "                    default=20000, type=int,\n",
    "                    help='Filename of test parquet file')\n",
    "args = parser.parse_args()\n",
    "\n",
    "PROJECT_ID = args.project_id\n",
    "GCS_BUCKET = args.gcs_bucket\n",
    "REGION = args.region\n",
    "TABLE_BQ = args.table_bq\n",
    "\n",
    "bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "try: \n",
    "    bucket = storage.Client().bucket(GCS_BUCKET)\n",
    "    bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)\n",
    "    logging.info(f\"Connection to BigQuery table {TABLE_BQ} and GCS Bucket {GCS_BUCKET} successfully.\")\n",
    "except:\n",
    "    logging.error(\"\")\n",
    "\n",
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    TABLE_BQ\n",
    ")\n",
    "rows = bqclient.list_rows(\n",
    "    table\n",
    ")\n",
    "masks = rows.to_dataframe(\n",
    "    create_bqstorage_client=True,\n",
    ")\n",
    "\n",
    "#reprocessing RLE data\n",
    "masks = masks[:args.n_truncate]\n",
    "masks.replace(to_replace=[None], value='', inplace=True)\n",
    "masks = masks.groupby(['ImageId'])['EncodedPixels'].apply(lambda x: ';'.join(x) if x is not None else ';'.join('')).reset_index()\n",
    "\n",
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: c_row.count(\";\"))\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "masks.drop(['ships'], axis=1, inplace=True)\n",
    "masks.EncodedPixels = masks.EncodedPixels.apply(lambda x: merge_rle_encode(x))\n",
    "\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['ships'])\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "logging.info(train_df.shape[0], 'training masks')\n",
    "logging.info(valid_df.shape[0], 'validation masks')\n",
    "\n",
    "train_df_balanced = pd.DataFrame()\n",
    "for ship_num in train_df['ships'].unique():\n",
    "    train_df_balanced = train_df_balanced.append(resample(train_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE))\n",
    "train_df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "valid_df_balanced = pd.DataFrame()\n",
    "for ship_num in valid_df['ships'].unique():\n",
    "    valid_df_balanced = valid_df_balanced.append(resample(valid_df.query(\"ships == {}\".format(ship_num)), n_samples=N_SAMPLE//10))\n",
    "    \n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = Path(f\"{timestamp}\")\n",
    "train_df_balanced.to_parquet(f\"train.parquet\")\n",
    "valid_df_balanced.to_parquet(f\"test.parquet\")\n",
    "\n",
    "try:\n",
    "    blob = bucket.blob(f\"output_path.join('train.parquet')\")\n",
    "    blob.upload_from_filename('train.parquet')\n",
    "    blob = bucket.blob(f\"output_path.join('test.parquet')\")\n",
    "    blob.upload_from_filename('test.parquet')\n",
    "    logging.info(\"File uploaded to GCS bucket successfully.\")\n",
    "except:\n",
    "    logging.error(\"File upload to GCS Bucket failed!\")\n",
    "\n",
    "#return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\"\n",
    "return f\"gs://{GCS_BUCKET}/train.parquet\", f\"gs://{GCS_BUCKET}/test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6263-da02-4695-8fee-0c40e7659a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
